{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 \n",
    "- \n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"ASSIGNMENT 1\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkSession classes \n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "from pyspark import SparkContext # Spark\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all 'Units' csv files into a single RDD object named as rd_units\n",
    "rd_units = sc.textFile('*_Units.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all 'Crash' csv files into a single RDD object named as rd_crash\n",
    "rd_crash = sc.textFile('*_Crash.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning rdds\n",
    "rd_units = rd_units.map(lambda x : (x.replace('\"','')))\n",
    "rd_crash = rd_crash.map(lambda x : (x.replace('\"','')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of records in rdd - rd_units is 153854\n",
      "\n",
      "Count of records in rdd - rd_crash is 72006\n",
      "\n",
      "Top 10 records of rdd - rd_units is [['2016-1-15/08/2019', '01', '0', 'SA', 'OMNIBUS', '2011', 'North', 'Male', '056', 'SA', 'HR', 'Full', 'Not Towing', 'Straight Ahead', '010', '5121', '', ''], ['2016-1-15/08/2019', '02', '1', '', 'Pedestrian on Road', '', 'East', 'Male', '072', '', '', '', '', 'Walking on Road', '', '5084', '', ''], ['2016-2-15/08/2019', '01', '0', 'SA', 'Motor Cars - Sedan', '2004', 'Unknown', 'Female', '023', 'SA', 'C ', 'Full', 'Not Towing', 'Straight Ahead', '001', '5087', '', ''], ['2016-2-15/08/2019', '02', '0', 'SA', 'Station Wagon', '2008', 'Unknown', 'Male', '040', 'SA', 'C ', 'Full', 'Not Towing', 'Straight Ahead', '001', '5084', '', ''], ['2016-3-15/08/2019', '01', '0', 'SA', 'RIGID TRUCK LGE GE 4.5T', '1990', 'South', 'Unknown', 'XXX', 'SA', 'MR', 'Provisional 2', 'Not Towing', 'Straight Ahead', '001', '5115', '', ''], ['2016-3-15/08/2019', '02', '0', 'SA', 'Panel Van', '2013', 'South', 'Male', '023', 'SA', 'C ', 'Full', 'Not Towing', 'Straight Ahead', '001', '5110', '', ''], ['2016-4-15/08/2019', '01', '0', 'SA', 'Station Wagon', '2002', 'East', 'Female', '033', 'SA', 'C ', 'Full', 'Not Towing', 'Straight Ahead', '001', '5169', '', ''], ['2016-4-15/08/2019', '02', '0', 'UNKNOWN', 'Other Defined Special Vehicle', 'XXXX', 'North', 'Unknown', 'XXX', 'UNKNOWN', 'XX', 'Unknown', 'Unknown', 'Reversing', '001', 'XXXX', '', ''], ['2016-5-15/08/2019', '01', '1', 'SA', 'Motor Cars - Sedan', '1997', 'South East', 'Male', '042', 'SA', 'C ', 'Full', 'Not Towing', 'Right Turn', '001', 'XXXX', '', ''], ['2016-5-15/08/2019', '02', '2', 'SA', 'Utility', '2015', 'North East', 'Male', '059', 'SA', 'MC', 'Full', 'Not Towing', 'Straight Ahead', '002', '5114', '', '']]\n",
      "\n",
      "Top 10 records of rdd - rd_crash is [['2019-1-8/07/2020', '2 Metropolitan', 'HAMPSTEAD GARDENS', '5086', 'CITY OF PORT ADELAIDE ENFIELD', '2', '0', '0', '0', '0', '2019', 'June', 'Wednesday', '11:15 am', '060', 'Cross Road', 'Straight road', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Right Angle', '01', 'Driver Rider', '1: PDO', 'Give Way Sign', '', '', '1331810.03', '1676603.26', '13318101676603'], ['2019-2-8/07/2020', '2 Metropolitan', 'DRY CREEK', '5094', 'CITY OF SALISBURY', '2', '0', '0', '0', '0', '2019', 'January', 'Tuesday', '12:49 am', '090', 'Divided Road', 'Straight road', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Night', 'Rear End', '02', 'Driver Rider', '1: PDO', 'No Control', '', '', '1328376.2', '1682942.63', '13283761682943'], ['2019-3-8/07/2020', '2 Metropolitan', 'MILE END', '5031', 'CITY OF WEST TORRENS', '2', '1', '0', '0', '1', '2019', 'January', 'Tuesday', '12:00 am', '060', 'Divided Road', 'Straight road', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Night', 'Hit Pedestrian', '01', 'Driver Rider', '2: MI', 'No Control', '', '', '1325819.68', '1670994.26', '13258201670994'], ['2019-4-8/07/2020', '2 Metropolitan', 'PARALOWIE', '5108', 'CITY OF SALISBURY', '2', '1', '0', '1', '0', '2019', 'January', 'Tuesday', '12:05 am', '050', 'Not Divided', 'CURVED', ' VIEW OPEN', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Night', 'Hit Fixed Object', '01', 'Driver Rider', '3: SI', 'No Control', '', '', '1328320.6', '1690237.08', '13283211690237'], ['2019-5-8/07/2020', '2 Metropolitan', 'MOUNT BARKER', '5251', 'DC MT.BARKER.                 ', '2', '0', '0', '0', '0', '2019', 'January', 'Tuesday', '05:15 am', '110', 'Divided Road', 'Straight road', 'Slope', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Night', 'Hit Animal', '02', 'Animal', '1: PDO', 'No Control', '', '', '1353279.99', '1655645.15', '13532801655645'], ['2019-6-8/07/2020', '2 Metropolitan', 'TORRENSVILLE', '5031', 'CITY OF WEST TORRENS', '2', '1', '0', '0', '1', '2019', 'January', 'Tuesday', '07:00 am', '050', 'Divided Road', 'Straight road', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Hit Fixed Object', '01', 'Driver Rider', '2: MI', 'No Control', '', '', '1324652.75', '1672027.64', '13246531672028'], ['2019-7-8/07/2020', '2 Metropolitan', 'BEDFORD PARK', '5042', 'CC MITCHAM.                   ', '2', '3', '0', '0', '3', '2019', 'January', 'Tuesday', '09:40 am', '050', 'Cross Road', 'Straight road', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Right Angle', '02', 'Driver Rider', '2: MI', 'Traffic Signals', '', '', '1325156.2', '1660414.38', '13251561660414'], ['2019-8-8/07/2020', '3 Country', 'WYE', '5291', 'DISTRICT COUNCIL OF GRANT', '1', '1', '0', '0', '1', '2019', 'January', 'Tuesday', '12:15 pm', '110', 'Not Divided', 'CURVED', ' VIEW OPEN', 'Level', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Roll Over', '01', 'Driver Rider', '2: MI', 'No Control', '', '', '1517347.8', '1321979.33', '15173481321979'], ['2019-9-8/07/2020', '3 Country', 'MOUNT GAMBIER', '5290', 'CC MT.GAMBIER.                ', '1', '1', '0', '0', '1', '2019', 'January', 'Tuesday', '11:45 am', '050', 'T-Junction', 'Straight road', 'Bottom of Hill', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Roll Over', '01', 'Driver Rider', '2: MI', 'Stop Sign', '', '', '1510220.05', '1340472.35', '15102201340472'], ['2019-10-8/07/2020', '3 Country', 'OVERLAND CORNER', '5330', 'THE BERRI BARMERA COUNCIL', '1', '1', '0', '1', '0', '2019', 'January', 'Tuesday', '12:30 pm', '080', 'Not Divided', 'CURVED', ' VIEW OPEN', 'Slope', 'Not Applicable', 'Sealed', 'Dry', 'Not Raining', 'Daylight', 'Roll Over', '01', 'Driver Rider', '3: SI', 'No Control', '', '', '1492599.9', '1749395.71', '14926001749396']]\n"
     ]
    }
   ],
   "source": [
    "#Removing header row for the rdd rd_units\n",
    "rd_units = rd_units.map(lambda line_units : line_units.split(','))\n",
    "header_units = rd_units.first()\n",
    "rd_units = rd_units.filter(lambda row_units : row_units != header_units )\n",
    "\n",
    "#Removing header row for the rdd rd_crash\n",
    "rd_crash = rd_crash.map(lambda line_crash : line_crash.split(','))\n",
    "header_crash = rd_crash.first()\n",
    "rd_crash = rd_crash.filter(lambda row_crash : row_crash != header_crash )\n",
    " \n",
    "#Printing count of rows in rdd rd_units\n",
    "print(\"Count of records in rdd - rd_units is \" + str(rd_units.count()) + \"\\n\")\n",
    "\n",
    "#Printing count of rows in rdd rd_crash\n",
    "print(\"Count of records in rdd - rd_crash is \" + str(rd_crash.count()) + \"\\n\")\n",
    "\n",
    "#Printing top 10 rows in rdd rd_units\n",
    "print(\"Top 10 records of rdd - rd_units is \" + str(rd_units.take(10)) + \"\\n\")\n",
    "\n",
    "#Printing top 10 rows in rdd rd_crash\n",
    "print(\"Top 10 records of rdd - rd_crash is \" + str(rd_crash.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in rdd - rd_units is 5\n",
      "\n",
      "Number of partitions in rdd - rd_crash is 5\n"
     ]
    }
   ],
   "source": [
    "#Printing number of paartitions in each rdd\n",
    "print(\"Number of partitions in rdd - rd_units is \" + str(rd_units.getNumPartitions()) + \"\\n\")\n",
    "print(\"Number of partitions in rdd - rd_crash is \" + str(rd_crash.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  ### The number of partitions in the above two rdds i.e. rd_units and rd_crash are 5 each. This can be seen from using the function .getNumPartitions() ###\n",
    "\n",
    "*  ### When we do not define the way of partitioning in an rdd, the rdd object itself by default does Round Robin equal partitioning. In this approach the rdd is divided into equal chunks of data and given to the different processing units. This way no processor sits idle while another processor is working on some task.  ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to parse a record in the rdd\n",
    "def parsingARecord(line):\n",
    "    array_line = line[0:9] + line[10:]\n",
    "    # Return a tuple with the lic state as first element and the remaining as the second element\n",
    "    return (line[9], array_line)\n",
    "\n",
    "rd_units_pair = rd_units.map(parsingARecord)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_partitions = 2\n",
    "\n",
    "#hash function to generate key value pairs\n",
    "def hash_function(key):\n",
    "    total = 0\n",
    "    if key == 'SA':\n",
    "        total = 0\n",
    "    else:\n",
    "        total = 1\n",
    "    return total\n",
    "\n",
    "\n",
    "rd_units_partitioned = rd_units_pair.partitionBy(no_of_partitions, hash_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### NUMBER OF PARTITIONS: 2\n",
      "Partition 0: 109684 records\n",
      "Partition 1: 44170 records\n"
     ]
    }
   ],
   "source": [
    "#A Function to print the data items in each RDD\n",
    "def print__the_partitions(data):\n",
    "    if isinstance(data, RDD):\n",
    "        numPartitions = data.getNumPartitions()\n",
    "        partitions = data.glom().collect()\n",
    "    else:\n",
    "        numPartitions = data.rdd.getNumPartitions()\n",
    "        partitions = data.rdd.glom().collect()\n",
    "    \n",
    "    print(f\"####### NUMBER OF PARTITIONS: {numPartitions}\")\n",
    "    for index, partition in enumerate(partitions):\n",
    "        # show partition if it is not empty\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\")\n",
    "\n",
    "\n",
    "print__the_partitions(rd_units_partitioned)            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### The above code displays the number of records in each partition. We can see that Partition 0 has 109,684 records and Partition 1 has 44,170 records. ###\n",
    "\n",
    "\n",
    "\n",
    "* ### As a result there is data skewness present as can be seen from the count of each partition. Partition 0(with key as SA) has 109,684 records and Partition 1 (with key other than SA) has 44,170 records. This tells us that one partition has almost 2.5 times the data as in the other partition. Due to the data skewness we could also see process skewness as the processors process the data. To manage this data skewness we can use a different approach of partitioning like Round robin or random equal  partitioning. This will divide the data equally so that no processor sits idle while other processors are busy doing tasks. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average age of male drivers is - 41.0\n",
      "\n",
      "Average age of female drivers is - 40.386\n"
     ]
    }
   ],
   "source": [
    "#creating separate rdds for male and female\n",
    "male = rd_units.filter(lambda x : x[7] in ['Male'] and x[4] != 'Pedestrian on Road')\n",
    "female = rd_units.filter(lambda x : x[7] in ['Female'] and x[4] != 'Pedestrian on Road')\n",
    "\n",
    "#getting just the male ages and storing in rdd male_rdd\n",
    "male_age = male.map(lambda x: x[8])\n",
    "\n",
    "\n",
    "#getting just the female ages and storing in rdd female_rdd\n",
    "female_age = female.map(lambda x: x[8])\n",
    "\n",
    "\n",
    "#function to find avergae of ages of male rdd and female rdd\n",
    "def find_average(rdd):\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    for val in rdd.collect():\n",
    "        try:\n",
    "            sum = sum + int(val)\n",
    "            if int(val) > 0:\n",
    "                count = count + 1\n",
    "        except:\n",
    "            sum = sum + 0\n",
    "    return(sum/count)\n",
    "\n",
    "male_age_avg = find_average(male_age)\n",
    "female_age_avg = find_average(female_age)\n",
    "\n",
    "print(\"Average age of male drivers is - \" + str(round(male_age_avg,3)) + \"\\n\" )\n",
    "print(\"Average age of female drivers is - \" + str(round(female_age_avg,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newest vehicle is - 2019\n",
      "\n",
      "Oldest vehicle is - 1900\n",
      "Records for Newest Vehicle - [('SA', '2019', 'Station Wagon'), ('SA', '2019', 'OMNIBUS'), ('SA', '2019', 'Motor Cars - Sedan')]\n",
      "\n",
      "Records for Oldest Vehicle - [('VIC', '1900', 'Motor Cycle'), ('SA', '1900', 'Motor Cycle'), ('SA', '1900', 'Motor Cycle')]\n"
     ]
    }
   ],
   "source": [
    "#capturing only the years into a separate rdd called year\n",
    "year = rd_units.map(lambda x: x[5])\n",
    "#year_final = year.map(lambda x: x.replace(\"\\\"\",''))\n",
    "\n",
    "#function to find maximum and minimum year values\n",
    "def find_maxmin_year(rdd):\n",
    "    max = 0\n",
    "    min = 0\n",
    "    try:\n",
    "        new_rdd = rdd.filter(lambda x : x.isdigit())\n",
    "        max = new_rdd.max(key=lambda x: x)\n",
    "        min = new_rdd.min(key=lambda x: x)\n",
    "    except:\n",
    "        pass\n",
    "    return(max,min)\n",
    "\n",
    "max_year,min_year = find_maxmin_year(year)\n",
    "\n",
    "print(\"Newest vehicle is - \" + str(max_year) + \"\\n\")\n",
    "print(\"Oldest vehicle is - \" + str(min_year))\n",
    "\n",
    "\n",
    "#finding records with max year\n",
    "all_max = rd_units.filter(lambda x : x[5] ==  max_year)\n",
    "\n",
    "#finding records with min year\n",
    "all_min = rd_units.filter(lambda x : x[5] ==  min_year)\n",
    "\n",
    "\n",
    "#displaying records with max year\n",
    "display_max = all_max.map(lambda field: (field[3] ,field[5],field[4]))\n",
    "print(\"Records for Newest Vehicle - \" + str(display_max.take(3)) + \"\\n\")\n",
    "\n",
    "\n",
    "#displaying records with min year\n",
    "display_min = all_min.map(lambda field: (field[3] ,field[5],field[4]))\n",
    "print(\"Records for Oldest Vehicle - \" + str(display_min.take(3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading units files to dataframe datarame_units\n",
    "dataframe_units = spark.read.csv('*_Units.csv',header = True)\n",
    "\n",
    "#loading crash files to dataframe datarame_crash\n",
    "dataframe_crash = spark.read.csv('*_Crash.csv',header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of dataframe - dataframe_units\n",
      "root\n",
      " |-- REPORT_ID: string (nullable = true)\n",
      " |-- Unit No: string (nullable = true)\n",
      " |-- No Of Cas: string (nullable = true)\n",
      " |-- Veh Reg State: string (nullable = true)\n",
      " |-- Unit Type: string (nullable = true)\n",
      " |-- Veh Year: string (nullable = true)\n",
      " |-- Direction Of Travel: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Lic State: string (nullable = true)\n",
      " |-- Licence Class: string (nullable = true)\n",
      " |-- Licence Type: string (nullable = true)\n",
      " |-- Towing: string (nullable = true)\n",
      " |-- Unit Movement: string (nullable = true)\n",
      " |-- Number Occupants: string (nullable = true)\n",
      " |-- Postcode: string (nullable = true)\n",
      " |-- Rollover: string (nullable = true)\n",
      " |-- Fire: string (nullable = true)\n",
      "\n",
      "\n",
      "\n",
      "Schema of dataframe - dataframe_crash\n",
      "root\n",
      " |-- REPORT_ID: string (nullable = true)\n",
      " |-- Stats Area: string (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Postcode: string (nullable = true)\n",
      " |-- LGA Name: string (nullable = true)\n",
      " |-- Total Units: string (nullable = true)\n",
      " |-- Total Cas: string (nullable = true)\n",
      " |-- Total Fats: string (nullable = true)\n",
      " |-- Total SI: string (nullable = true)\n",
      " |-- Total MI: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Area Speed: string (nullable = true)\n",
      " |-- Position Type: string (nullable = true)\n",
      " |-- Horizontal Align: string (nullable = true)\n",
      " |-- Vertical Align: string (nullable = true)\n",
      " |-- Other Feat: string (nullable = true)\n",
      " |-- Road Surface: string (nullable = true)\n",
      " |-- Moisture Cond: string (nullable = true)\n",
      " |-- Weather Cond: string (nullable = true)\n",
      " |-- DayNight: string (nullable = true)\n",
      " |-- Crash Type: string (nullable = true)\n",
      " |-- Unit Resp: string (nullable = true)\n",
      " |-- Entity Code: string (nullable = true)\n",
      " |-- CSEF Severity: string (nullable = true)\n",
      " |-- Traffic Ctrls: string (nullable = true)\n",
      " |-- DUI Involved: string (nullable = true)\n",
      " |-- Drugs Involved: string (nullable = true)\n",
      " |-- ACCLOC_X: string (nullable = true)\n",
      " |-- ACCLOC_Y: string (nullable = true)\n",
      " |-- UNIQUE_LOC: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Displaying schema of the two dataframes\n",
    "print(\"Schema of dataframe - dataframe_units\")\n",
    "dataframe_units.printSchema()\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Schema of dataframe - dataframe_crash\")\n",
    "dataframe_crash.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "|           REPORT_ID|Stats Area|  Suburb|Postcode|        LGA Name|Total Units|Total Cas|Total Fats|Total SI|Total MI|Year|   Month|     Day|    Time|Area Speed|Position Type|Horizontal Align|Vertical Align|          Other Feat|Road Surface|Moisture Cond|Weather Cond|DayNight|    Crash Type|Unit Resp| Entity Code|CSEF Severity|  Traffic Ctrls|DUI Involved|Drugs Involved|  ACCLOC_X|  ACCLOC_Y|    UNIQUE_LOC|\n",
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "| 2018-601-17/01/2020|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          8|        4|         0|       2|       2|2018| January|  Sunday|09:12 pm|       050|  Not Divided|   Straight road|         Level|      Not Applicable|      Sealed|          Dry| Not Raining|   Night|Hit Pedestrian|       01|Driver Rider|        3: SI|     No Control|        null|          null|1329806.36|1670224.76|13298061670225|\n",
      "|2017-1613-15/08/2019|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          2|        4|         0|       0|       4|2017|February|Saturday|04:00 pm|       050|   Cross Road|   Straight road|         Level|      Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Right Turn|       01|Driver Rider|        2: MI|Traffic Signals|        null|          null|1327951.24|1669556.92|13279511669557|\n",
      "|2017-12182-15/08/...|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          6|        5|         0|       1|       4|2017|December|Saturday|04:08 pm|       050|   Cross Road|   Straight road|         Level|      Not Applicable|      Sealed|          Wet| Not Raining|Daylight|Hit Pedestrian|       01|Driver Rider|        3: SI|Traffic Signals|        null|          null| 1329016.2|1670995.07|13290161670995|\n",
      "|2019-10404-8/07/2020|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          4|        6|         0|       0|       6|2019| October|  Monday|08:20 am|       060| Divided Road|   Straight road|         Level|Driveway or Entrance|      Sealed|          Dry| Not Raining|Daylight|    Right Turn|       01|Driver Rider|        2: MI|     No Control|        null|          null|1327088.72|1670880.07|13270891670880|\n",
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Crash events in ADELAIDE where TOTAL CASUALITIES are more than 3\n",
    "\n",
    "#casting the column Total Cas to int and then filtering greaer than 3 values\n",
    "cas = dataframe_crash.filter(col(\"Suburb\").isin ('ADELAIDE') & (col(\"Total Cas\").cast(\"int\") > 3))\n",
    "cas.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+------+\n",
      "|           REPORT_ID|    Stats Area|         Suburb|Postcode|            LGA Name|Total Units|Total Cas|Total Fats|Total SI|Total MI|Year|   Month|      Day|    Time|Area Speed|Position Type|    Horizontal Align|Vertical Align|    Other Feat|Road Surface|Moisture Cond|Weather Cond|DayNight| Crash Type|Unit Resp| Entity Code|CSEF Severity|  Traffic Ctrls|DUI Involved|Drugs Involved|  ACCLOC_X|  ACCLOC_Y|    UNIQUE_LOC|CasTmp|\n",
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+------+\n",
      "| 2017-288-15/08/2019|2 Metropolitan|     PARA HILLS|    5096|   CITY OF SALISBURY|          2|       11|         0|       1|      10|2017| January|Wednesday|01:13 pm|       060|   T-Junction|       Straight road| Crest of Hill|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|Right Angle|       01|Driver Rider|        3: SI|      Stop Sign|        null|          null| 1334428.9|1683032.96|13344291683033|    11|\n",
      "|2016-3035-15/08/2019|2 Metropolitan|        HACKHAM|    5163| CITY OF ONKAPARINGA|          3|        9|         3|       5|       1|2016| January| Saturday|11:50 am|       080|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight| Right Turn|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1320361.49|1645195.63|13203611645196|     9|\n",
      "|2016-6630-15/08/2019|2 Metropolitan|  KANGAROO FLAT|    5118|LIGHT REGIONAL CO...|          3|        9|         0|       2|       7|2016|   April|Wednesday|09:00 pm|       100|  Not Divided|CURVED, VIEW OBSC...|         Level|Not Applicable|      Sealed|          Dry| Not Raining|   Night|    Head On|       01|Driver Rider|        3: SI|     No Control|        null|          null|1339316.32|1710314.92|13393161710315|     9|\n",
      "|2019-11734-8/07/2020|2 Metropolitan|          STURT|    5047|CC MARION.       ...|          2|        9|         0|       1|       8|2019|November|   Sunday|07:25 pm|       060|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight| Right Turn|       02|Driver Rider|        3: SI|Traffic Signals|        null|          null|1324428.84|1659884.95|13244291659885|     9|\n",
      "|2016-14407-15/08/...|     3 Country|      STOCKWELL|    5355|THE BAROSSA COUNCIL.|          2|        8|         1|       6|       1|2016| October|   Sunday|03:46 pm|       100|  Not Divided|       Straight road| Crest of Hill|Not Applicable|    Unsealed|          Dry| Not Raining|Daylight|    Head On|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1373964.45|1723462.57|13739641723463|     8|\n",
      "|2016-7073-15/08/2019|     3 Country|       MERRITON|    5523|PT.PIRIE CITY & D...|          2|        8|         4|       3|       1|2016|   April|   Sunday|12:35 pm|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Head On|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1293759.89|1840109.96|12937601840110|     8|\n",
      "|2015-2823-21/08/2019|     3 Country|         HAWKER|    5434|THE FLINDERS RANG...|          1|        8|         0|       0|       8|2015|   March|   Monday|06:00 pm|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|  Roll Over|       01|Driver Rider|        2: MI|     No Control|        null|          null|1315077.61|2022309.34|13150782022309|     8|\n",
      "|2015-12591-21/08/...|     3 Country|        MALLALA|    5502|DC MALLALA.      ...|          2|        7|         0|       2|       5|2015| October|   Sunday|02:30 pm|       100|   Cross Road|       Straight road|         Level|Not Applicable|    Unsealed|          Dry| Not Raining|Daylight|Right Angle|       01|Driver Rider|        3: SI|  Give Way Sign|        null|          null|1325122.01|1724860.95|13251221724861|     7|\n",
      "|2015-13713-21/08/...|2 Metropolitan|ELIZABETH GROVE|    5112|   CITY OF PLAYFORD.|          2|        7|         0|       0|       7|2015|November|   Friday|03:42 pm|       080|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|   Rear End|       01|Driver Rider|        2: MI|     No Control|        null|          null|1336118.68|1691385.65|13361191691386|     7|\n",
      "|2015-6965-21/08/2019|     3 Country|       BEAUFORT|    5550|YORKE PENINSULA C...|          3|        7|         3|       4|       0|2015|    June|   Monday|11:13 am|       100|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Head On|       09|       Other|     4: Fatal|     No Control|        null|          null|1287930.19|1761652.36|12879301761652|     7|\n",
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Casting Total Cas to int type and creating a new column CasTmp\n",
    "dataframe_crash = dataframe_crash.withColumn(\"CasTmp\", dataframe_crash[\"Total Cas\"].cast(IntegerType()))\n",
    "\n",
    "#Finding top 10 rows with highest casualities by sorting in descending order and displaying top 10 records\n",
    "highest_cas = dataframe_crash.sort('CasTmp', ascending=False)\n",
    "highest_cas.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|          Crash Type|Total Fatalities|\n",
      "+--------------------+----------------+\n",
      "|           Roll Over|            57.0|\n",
      "|  Hit Object on Road|             2.0|\n",
      "|      Hit Pedestrian|            70.0|\n",
      "|    Hit Fixed Object|           152.0|\n",
      "|               Other|             2.0|\n",
      "|          Side Swipe|            20.0|\n",
      "|             Head On|            86.0|\n",
      "|  Hit Parked Vehicle|             9.0|\n",
      "|          Right Turn|            18.0|\n",
      "|            Rear End|            16.0|\n",
      "|          Hit Animal|             4.0|\n",
      "|Left Road - Out o...|             1.0|\n",
      "|         Right Angle|            45.0|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the dataset by Crash Type and sum the total number of fatalities\n",
    "agg_attribute1 = 'Crash Type'\n",
    "sum_attribute = 'Total Fats'\n",
    "\n",
    "#group by Crash Type and display sum of fatalities in a column 'Total Fatalities'\n",
    "fats = dataframe_crash.groupby(agg_attribute1).agg(F.sum(sum_attribute).alias('Total Fatalities'))\n",
    "fats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|             Suburb|Total Casualities|\n",
      "+-------------------+-----------------+\n",
      "|      FLINDERS PARK|              8.0|\n",
      "|       POOGINAGORIC|              1.0|\n",
      "|     TEA TREE GULLY|              1.0|\n",
      "|            HACKHAM|              3.0|\n",
      "|   MEDINDIE GARDENS|              0.0|\n",
      "|           WISANGER|              1.0|\n",
      "|            CUMMINS|              0.0|\n",
      "|       BASKET RANGE|              0.0|\n",
      "|MURRAY BRIDGE SOUTH|              0.0|\n",
      "|      GILLES PLAINS|              7.0|\n",
      "|             HAWKER|              0.0|\n",
      "|           BEAUFORT|              1.0|\n",
      "|             MAGILL|              7.0|\n",
      "|            ECHUNGA|              1.0|\n",
      "|            CULTANA|              1.0|\n",
      "|        EDWARDSTOWN|              6.0|\n",
      "|        RISDON PARK|              1.0|\n",
      "|          THORNGATE|              1.0|\n",
      "|       ANDREWS FARM|             12.0|\n",
      "|       TORRENSVILLE|              5.0|\n",
      "+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the dataset by 'Suburb' and sum the total number of casualities\n",
    "\n",
    "agg_attribute2 = 'Suburb'\n",
    "sum_attribute = 'Total Cas'\n",
    "\n",
    "#joining the two dataframes on right join (as we need to fetch records for eaach suburb)\n",
    "df_units_inner_crash = dataframe_units.join(dataframe_crash,dataframe_units.REPORT_ID==dataframe_crash.REPORT_ID,how='right')\n",
    "\n",
    "#group by Suburb and display sum of casualities in a column 'Total Casualities'\n",
    "df_count = df_units_inner_crash.filter(df_units_inner_crash['Licence Type'] == 'Unlicenced')\\\n",
    "                        .groupby(agg_attribute2).agg(F.sum(sum_attribute).alias('Total Casualities'))\n",
    "\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|CSEF Severity|Total|\n",
      "+-------------+-----+\n",
      "|       1: PDO|46696|\n",
      "|        2: MI|21881|\n",
      "|        3: SI| 2978|\n",
      "|     4: Fatal|  451|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the dataset by 'CSEF Severity' and count the total number of records\n",
    "\n",
    "agg_attribute3 = 'CSEF Severity'\n",
    "\n",
    "severity = dataframe_crash.groupby(agg_attribute3).agg(F.count(agg_attribute3).alias('Total'))\\\n",
    "                    .orderBy('Total',ascending = False)\n",
    "severity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### The most common CSEF Severity is of type 1.PDO i.e. Public Damage Only occuring the most times i.e. 46696 times. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+\n",
      "|CSEF Severity|Count|Percentage|\n",
      "+-------------+-----+----------+\n",
      "|     4: Fatal|   82|      6.54|\n",
      "|       1: PDO|  176|     14.04|\n",
      "|        3: SI|  247|      19.7|\n",
      "|        2: MI|  749|     59.73|\n",
      "+-------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the dataset by 'CSEF Severity' and count the total number of records and show its percentage as well\n",
    "\n",
    "agg_attribute4 =  'CSEF Severity'\n",
    "\n",
    "#counting the number of records where drugs involved is true\n",
    "total_drugs = dataframe_crash.filter(dataframe_crash['Drugs Involved'] == 'Y').count()\n",
    "\n",
    "#Grouping by CSEF Severity and finding count and Percentage of each severity type\n",
    "positive_drug = dataframe_crash.filter(dataframe_crash['Drugs Involved'] == 'Y')\\\n",
    "                    .groupBy(agg_attribute4) \\\n",
    "                      .count().withColumnRenamed('count', 'Count') \\\n",
    "                        .withColumn('Percentage', sf.round((F.col('Count') / total_drugs) * 100,2) )\\\n",
    "                            .orderBy('Percentage')\n",
    "  \n",
    "positive_drug.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+\n",
      "|CSEF Severity|Count|Percentage|\n",
      "+-------------+-----+----------+\n",
      "|     4: Fatal|   79|      3.51|\n",
      "|        3: SI|  259|     11.52|\n",
      "|        2: MI|  737|     32.78|\n",
      "|       1: PDO| 1173|     52.18|\n",
      "+-------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the dataset by 'CSEF Severity' and count the total number of records and show its percentage as well\n",
    "\n",
    "agg_attribute5 =  'CSEF Severity'\n",
    "\n",
    "#counting the number of records where alcohol involved is true\n",
    "total_alcohol = dataframe_crash.filter(dataframe_crash['DUI Involved'] == 'Y').count()\n",
    "\n",
    "#Grouping by CSEF Severity and finding count and Percentage of each severity type\n",
    "positive_alcohol = dataframe_crash.filter(dataframe_crash['DUI Involved'] == 'Y')\\\n",
    "                    .groupBy(agg_attribute5) \\\n",
    "                      .count().withColumnRenamed('count', 'Count') \\\n",
    "                        .withColumn('Percentage', sf.round((F.col('Count') / total_alcohol) * 100,2) )\\\n",
    "                            .orderBy('Percentage')\n",
    "  \n",
    "positive_alcohol.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+\n",
      "|CSEF Severity|Count|Percentage|\n",
      "+-------------+-----+----------+\n",
      "|       1: PDO|   24|     13.71|\n",
      "|     4: Fatal|   27|     15.43|\n",
      "|        3: SI|   35|      20.0|\n",
      "|        2: MI|   89|     50.86|\n",
      "+-------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the dataset by 'CSEF Severity' and count the total number of records and show its percentage as well\n",
    "\n",
    "agg_attribute6 =  'CSEF Severity'\n",
    "\n",
    "#counting the number of records where both drugs and alcohol involved is true\n",
    "total_drug_alcohol = dataframe_crash.filter((dataframe_crash['DUI Involved'] == 'Y') & (dataframe_crash['Drugs Involved'] == 'Y')).count()\n",
    "\n",
    "\n",
    "#Grouping by CSEF Severity and finding count and Percentage of each severity type\n",
    "positive_drugs_alcohol = dataframe_crash.filter((dataframe_crash['DUI Involved'] == 'Y') & (dataframe_crash['Drugs Involved'] == 'Y'))\\\n",
    "                    .groupBy(agg_attribute6) \\\n",
    "                      .count().withColumnRenamed('count', 'Count') \\\n",
    "                        .withColumn('Percentage', sf.round((F.col('Count') / total_drug_alcohol) * 100,2) )\\\n",
    "                            .orderBy('Percentage')\n",
    "\n",
    "positive_drugs_alcohol.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+----------+\n",
      "|CSEF Severity|Count|Percentage|\n",
      "+-------------+-----+----------+\n",
      "|     4: Fatal|  317|      0.46|\n",
      "|        3: SI| 2507|      3.65|\n",
      "|        2: MI|20484|     29.83|\n",
      "|       1: PDO|45371|     66.06|\n",
      "+-------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the dataset by 'CSEF Severity' and count the total number of records and show its percentage as well\n",
    "\n",
    "agg_attribute7 =  'CSEF Severity'\n",
    "\n",
    "#counting the number of records where both drugs and alcohol involved is false\n",
    "no_drug_alcohol = dataframe_crash.filter((dataframe_crash['DUI Involved'].isNull() ) & (dataframe_crash['Drugs Involved'].isNull())).count()\n",
    "\n",
    "\n",
    "#Grouping by CSEF Severity and finding count and Percentage of each severity type\n",
    "negative_drugs_alcohol = dataframe_crash.filter((dataframe_crash['DUI Involved'].isNull()) & (dataframe_crash['Drugs Involved'].isNull()))\\\n",
    "                    .groupBy(agg_attribute7) \\\n",
    "                      .count().withColumnRenamed('count', 'Count') \\\n",
    "                        .withColumn('Percentage', sf.round((F.col('Count') / no_drug_alcohol) * 100 ,2))\\\n",
    "                            .orderBy('Percentage')\n",
    "\n",
    "\n",
    "negative_drugs_alcohol.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### From the above 4 scenarios we can see that PDO and MI type of severity are seen the most times in terms of percentages. Fatal percentage is usually very less in all the four scenarios when compared to the other types i.e. MI, SI, PDO. However Fatal type is seen more when drugs and alcohol are involved. Also when none of drugs and alcohl are involved then also we can see a high percentage of property damage which is the maximum when comparing all the scenarios.###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 using DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displying records for each unit for Adelaide Suburb\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|                Date|    Time|Total Cas|    Sex| Age|Licence Type|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 056|        Full|\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 072|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1|   Male| 056|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1| Female| 027|        null|\n",
      "|2016-November-Tue...|05:00 pm|        0| Female| 032|        Full|\n",
      "|2016-November-Tue...|05:00 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 022|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 020|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|   Male| 042|        Full|\n",
      "|2016-November-Monday|11:26 pm|        0|   null|null|        null|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 026|     Unknown|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 038|        Full|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 036|        Full|\n",
      "|2016-November-Tue...|05:05 pm|        0|   Male| 025|     Unknown|\n",
      "|2016-November-Tue...|05:05 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|03:30 pm|        1| Female| 065|        Full|\n",
      "|2016-November-Wed...|03:30 pm|        1|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|02:20 pm|        0|   Male| 063|        Full|\n",
      "|2016-November-Wed...|02:20 pm|        0|   null|null|        null|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 16.5 ms, sys: 390 µs, total: 16.9 ms\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2.4 - 1\n",
    "\n",
    "# USING DATAFRAME\n",
    "\n",
    "#joining dataframes on left join to calculate records for each unit\n",
    "new_join = dataframe_units.join(dataframe_crash,dataframe_units.REPORT_ID==dataframe_crash.REPORT_ID,how='left')\n",
    "\n",
    "\n",
    "#filtering for suburb ADELAIDE\n",
    "new_df_crash = new_join.filter(new_join['Suburb'] =='ADELAIDE')\\\n",
    "            .withColumn('Date',sf.concat(sf.col('Year'),sf.lit('-'), sf.col('Month'),sf.lit('-'), sf.col('Day')))\\\n",
    "                .select('Date','Time','Total Cas','Sex','Age', 'Licence Type')\n",
    "\n",
    "print(\"Displying records for each unit for Adelaide Suburb\")\n",
    "new_df_crash.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displying records for each unit for Adelaide Suburb\n",
      "[('2016-November-Wednesday', '04:26 pm', '0', 'Male', '017', 'Unknown'), ('2016-November-Wednesday', '04:26 pm', '0', 'Male', '025', 'Unknown'), ('2016-December-Friday', '11:30 am', '0', 'Male', '080', 'Full'), ('2016-December-Friday', '11:30 am', '0', 'Male', '048', 'Full'), ('2016-December-Saturday', '07:40 am', '0', 'Male', '032', 'Full'), ('2016-December-Saturday', '07:40 am', '0', 'Unknown', 'XXX', 'Unknown'), ('2016-December-Friday', '05:30 pm', '0', 'Female', '058', 'Full'), ('2016-December-Friday', '05:30 pm', '0', 'Male', '041', 'Full'), ('2016-December-Wednesday', '04:20 pm', '0', 'Female', '045', 'Full'), ('2016-December-Wednesday', '04:20 pm', '0', 'Male', '027', 'Full')]\n",
      "CPU times: user 83.7 ms, sys: 13.2 ms, total: 96.9 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2.4 - 1\n",
    "\n",
    "# USING RDD\n",
    "\n",
    "\n",
    "#function to create Report_ID as key and rest elements as value\n",
    "def parseNewRecord(line):\n",
    "    array_line = line[1:]\n",
    "    # Return a tuple with the REPORT_ID as first element and the remaining as the second element\n",
    "    return (line[0], array_line)\n",
    "\n",
    "\n",
    "#creating new rdd for units\n",
    "rdd_new = rd_units.map(parseNewRecord)\n",
    "\n",
    "#creating new rdd for crash\n",
    "rdd2_new = rd_crash.map(parseNewRecord)\n",
    "\n",
    "\n",
    "#joining the two rdds\n",
    "joined_rdd = rdd_new.join(rdd2_new)\n",
    "\n",
    "#filtering data by ADELAIDE\n",
    "filtered_rdd = joined_rdd.filter(lambda x : x[1][1][1] == 'ADELAIDE')\n",
    "\n",
    "#Displying the records\n",
    "filtered_rdd = filtered_rdd.map(lambda field: (field[1][1][9] + '-' +  field[1][1][10] + '-' + field[1][1][11] ,\\\n",
    "                                    field[1][1][12], field[1][1][5],field[1][0][6],field[1][0][7],field[1][0][10]))\n",
    "\n",
    "#displaying only 10 records\n",
    "print(\"Displying records for each unit for Adelaide Suburb\")\n",
    "print(filtered_rdd.take(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1 using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displying records for each unit for Adelaide Suburb\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|                 Day|    Time|Total Cas|    Sex| Age|Licence Type|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 056|        Full|\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 072|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1|   Male| 056|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1| Female| 027|        null|\n",
      "|2016-November-Tue...|05:00 pm|        0| Female| 032|        Full|\n",
      "|2016-November-Tue...|05:00 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 022|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 020|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|   Male| 042|        Full|\n",
      "|2016-November-Monday|11:26 pm|        0|   null|null|        null|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 026|     Unknown|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 038|        Full|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 036|        Full|\n",
      "|2016-November-Tue...|05:05 pm|        0|   Male| 025|     Unknown|\n",
      "|2016-November-Tue...|05:05 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|03:30 pm|        1| Female| 065|        Full|\n",
      "|2016-November-Wed...|03:30 pm|        1|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|02:20 pm|        0|   Male| 063|        Full|\n",
      "|2016-November-Wed...|02:20 pm|        0|   null|null|        null|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 5.41 ms, sys: 116 µs, total: 5.53 ms\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2.4 - 1\n",
    "\n",
    "# USING SQL\n",
    "\n",
    "dataframe_crash.createOrReplaceTempView(\"sql_crash\")\n",
    "dataframe_units.createOrReplaceTempView(\"sql_units\")\n",
    "\n",
    "## Join units and crash using SQL\n",
    "sql_units_left_crash = spark.sql('''\n",
    "  SELECT CONCAT(c.Year, '-',  c.Month , '-' , c.Day) as Day, c.Time, c.`Total Cas` , u.Sex, u.Age, u.`Licence Type`\n",
    "  FROM sql_units u LEFT JOIN sql_crash c\n",
    "  ON c.REPORT_ID = u.REPORT_ID\n",
    "  WHERE c.Suburb like 'ADELAIDE'\n",
    "''')\n",
    "\n",
    "print(\"Displying records for each unit for Adelaide Suburb\")\n",
    "sql_units_left_crash.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2 using DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displying records when Licence Type was Unlicenced\n",
      "+-------------------+-----------------+\n",
      "|             Suburb|Total Casualities|\n",
      "+-------------------+-----------------+\n",
      "|      FLINDERS PARK|                8|\n",
      "|       POOGINAGORIC|                1|\n",
      "|     TEA TREE GULLY|                1|\n",
      "|            HACKHAM|                3|\n",
      "|   MEDINDIE GARDENS|                0|\n",
      "|           WISANGER|                1|\n",
      "|            CUMMINS|                0|\n",
      "|       BASKET RANGE|                0|\n",
      "|MURRAY BRIDGE SOUTH|                0|\n",
      "|      GILLES PLAINS|                7|\n",
      "|             HAWKER|                0|\n",
      "|           BEAUFORT|                1|\n",
      "|             MAGILL|                7|\n",
      "|            ECHUNGA|                1|\n",
      "|            CULTANA|                1|\n",
      "|        EDWARDSTOWN|                6|\n",
      "|        RISDON PARK|                1|\n",
      "|          THORNGATE|                1|\n",
      "|       ANDREWS FARM|               12|\n",
      "|       TORRENSVILLE|                5|\n",
      "+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 10.5 ms, sys: 4.12 ms, total: 14.7 ms\n",
      "Wall time: 3.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 2.4 - 2\n",
    "\n",
    "# USING DATAFRAME\n",
    "\n",
    "\n",
    "#joining dataframes on left join to calculate records for each subur\n",
    "join = dataframe_crash.join(dataframe_units,dataframe_units.REPORT_ID==dataframe_crash.REPORT_ID,how='left')\n",
    "\n",
    "join2 = join.withColumn(\"CasTmp\", join[\"Total Cas\"].cast(IntegerType()))\n",
    "\n",
    "# Aggregate the dataset by 'Suburb' and sum the total number of casualities\n",
    "agg_attribute8 = 'Suburb'\n",
    "cas_sum = 'CasTmp'\n",
    "\n",
    "new_df_unlicenced = join2.filter(join2['Licence Type'] =='Unlicenced')\\\n",
    "                        .groupBy(agg_attribute8).agg(F.sum(cas_sum).alias('Total Casualities'))\n",
    "\n",
    "\n",
    "print(\"Displying records when Licence Type was Unlicenced\")\n",
    "new_df_unlicenced.show()\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 using RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displying records when Licence Type was Unlicenced\n",
      "[('ONKAPARINGA HILLS', 7), ('FIRLE', 6), ('RIVERGLADES', 1), ('SALISBURY PARK', 0), ('HAPPY VALLEY', 5), ('MUNNO PARA WEST', 1), ('HEATHPOOL', 3), ('NORTH PLYMPTON', 1), ('WHITES FLAT', 0), ('CRAIGMORE', 2)]\n",
      "CPU times: user 92 ms, sys: 8.79 ms, total: 101 ms\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2.4 - 2\n",
    "\n",
    "# USING RDD\n",
    "\n",
    "\n",
    "#function to create Report_ID as key and rest elements as value\n",
    "def parseNewRecord(line):\n",
    "    array_line = line[1:]\n",
    "    # Return a tuple with the REPORT_ID as first element and the remaining as the second element\n",
    "    return (line[0], array_line)\n",
    "\n",
    "\n",
    "#creating new rdd for units\n",
    "rdd_new = rd_units.map(parseNewRecord)\n",
    "\n",
    "#creating new rdd for crash\n",
    "rdd2_new = rd_crash.map(parseNewRecord)\n",
    "\n",
    "\n",
    "\n",
    "#joining the two rdds\n",
    "joined_rdd2 = rdd2_new.join(rdd_new)\n",
    "\n",
    "\n",
    "#filtering data by Unlicenced\n",
    "joined_rdd2 = joined_rdd2.filter(lambda x : x[1][1][10] == 'Unlicenced')\n",
    "\n",
    "#Displying the records\n",
    "new_joined_rdd = joined_rdd2.map(lambda x : (x[1][0][1] , int(x[1][0][5])))\n",
    "\n",
    "#using mapValues() to get sum of each suburb after grouping by suburb\n",
    "final_rdd_filtered = new_joined_rdd.groupByKey().mapValues(lambda f : sum(f))\n",
    "\n",
    "#displaying only 10 records\n",
    "print(\"Displying records when Licence Type was Unlicenced\")\n",
    "print(final_rdd_filtered.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2 using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displying records when Licence Type was Unlicenced\n",
      "+-------------------+------------------------------+\n",
      "|             Suburb|sum(CAST(Total Cas AS DOUBLE))|\n",
      "+-------------------+------------------------------+\n",
      "|      FLINDERS PARK|                           8.0|\n",
      "|       POOGINAGORIC|                           1.0|\n",
      "|     TEA TREE GULLY|                           1.0|\n",
      "|            HACKHAM|                           3.0|\n",
      "|   MEDINDIE GARDENS|                           0.0|\n",
      "|           WISANGER|                           1.0|\n",
      "|            CUMMINS|                           0.0|\n",
      "|       BASKET RANGE|                           0.0|\n",
      "|MURRAY BRIDGE SOUTH|                           0.0|\n",
      "|      GILLES PLAINS|                           7.0|\n",
      "|             HAWKER|                           0.0|\n",
      "|           BEAUFORT|                           1.0|\n",
      "|             MAGILL|                           7.0|\n",
      "|            ECHUNGA|                           1.0|\n",
      "|            CULTANA|                           1.0|\n",
      "|        EDWARDSTOWN|                           6.0|\n",
      "|        RISDON PARK|                           1.0|\n",
      "|          THORNGATE|                           1.0|\n",
      "|       ANDREWS FARM|                          12.0|\n",
      "|       TORRENSVILLE|                           5.0|\n",
      "+-------------------+------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 8.28 ms, sys: 0 ns, total: 8.28 ms\n",
      "Wall time: 3.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2.4 - 2\n",
    "# USING SQL\n",
    "\n",
    "\n",
    "dataframe_crash.createOrReplaceTempView(\"sql_crash\")\n",
    "dataframe_units.createOrReplaceTempView(\"sql_units\")\n",
    "\n",
    "\n",
    "## Join units and crash using SQL\n",
    "sql_crash_left_units = spark.sql('''\n",
    "  SELECT c.Suburb , sum(c.`Total Cas`)\n",
    "  FROM sql_crash c LEFT JOIN sql_units u\n",
    "  ON c.REPORT_ID = u.REPORT_ID\n",
    "  WHERE u.`Licence Type` like 'Unlicenced'\n",
    "  GROUP BY c.Suburb\n",
    "''')\n",
    "\n",
    "print(\"Displying records when Licence Type was Unlicenced\")\n",
    "sql_crash_left_units.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
